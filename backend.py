"""
AI ë³´ì»¬ ë¦¬ë¬´ë²„ ë°±ì—”ë“œ ì„œë²„
ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•œ ê³ í’ˆì§ˆ ë³´ì»¬/ì½”ëŸ¬ìŠ¤ ì œê±°
"""

import os
import io
import numpy as np
import librosa
import soundfile as sf
import tensorflow as tf
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import tempfile
from scipy.signal import stft, istft
import warnings
warnings.filterwarnings('ignore')

app = Flask(__name__)
CORS(app)  # CORS í—ˆìš©

class VocalSeparationModel:
    def __init__(self):
        self.vocal_model = None
        self.chorus_model = None
        self.sample_rate = 22050
        self.n_fft = 2048
        self.hop_length = 512
        self.is_loaded = False
        
    def load_models(self):
        """ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë“œ"""
        try:
            print("AI ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.vocal_model = self._create_vocal_model()
            self.chorus_model = self._create_chorus_model()
            self.is_loaded = True
            print("AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.is_loaded = False
    
    def _create_vocal_model(self):
        """ë³´ì»¬ ë¶„ë¦¬ U-Net ëª¨ë¸"""
        input_shape = (None, 1025, 2)  # (time, freq, channels)
        
        inputs = tf.keras.Input(shape=input_shape)
        
        # Encoder
        conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
        conv1 = tf.keras.layers.BatchNormalization()(conv1)
        pool1 = tf.keras.layers.MaxPooling2D((2, 1))(conv1)
        
        conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)
        conv2 = tf.keras.layers.BatchNormalization()(conv2)
        pool2 = tf.keras.layers.MaxPooling2D((2, 1))(conv2)
        
        conv3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)
        conv3 = tf.keras.layers.BatchNormalization()(conv3)
        pool3 = tf.keras.layers.MaxPooling2D((2, 1))(conv3)
        
        # Bottleneck
        conv4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)
        conv4 = tf.keras.layers.BatchNormalization()(conv4)
        
        # Decoder
        up5 = tf.keras.layers.UpSampling2D((2, 1))(conv4)
        merge5 = tf.keras.layers.Concatenate()([up5, conv3])
        conv5 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(merge5)
        
        up6 = tf.keras.layers.UpSampling2D((2, 1))(conv5)
        merge6 = tf.keras.layers.Concatenate()([up6, conv2])
        conv6 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(merge6)
        
        up7 = tf.keras.layers.UpSampling2D((2, 1))(conv6)
        merge7 = tf.keras.layers.Concatenate()([up7, conv1])
        conv7 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(merge7)
        
        # Output mask
        mask = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(conv7)
        
        model = tf.keras.Model(inputs=inputs, outputs=mask)
        model.compile(optimizer='adam', loss='mse')
        
        return model
    
    def _create_chorus_model(self):
        """ì½”ëŸ¬ìŠ¤ ì œê±° ê³ ê¸‰ ëª¨ë¸"""
        input_shape = (None, 1025, 2)
        
        inputs = tf.keras.Input(shape=input_shape)
        
        # Multi-scale feature extraction
        conv1_3 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)
        conv1_5 = tf.keras.layers.Conv2D(16, (5, 5), activation='relu', padding='same')(inputs)
        conv1_7 = tf.keras.layers.Conv2D(16, (7, 7), activation='relu', padding='same')(inputs)
        
        merged = tf.keras.layers.Concatenate()([conv1_3, conv1_5, conv1_7])
        
        # Deep processing
        x = tf.keras.layers.Dropout(0.2)(x)
        
        x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)
        x = tf.keras.layers.BatchNormalization()(x)
        
        x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Dropout(0.2)(x)
        
        # Attention mechanism
        attention = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(x)
        attended = tf.keras.layers.Multiply()([x, attention])
        
        # Final mask
        mask = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(attended)
        
        model = tf.keras.Model(inputs=inputs, outputs=mask)
        model.compile(optimizer='adam', loss='mse')
        
        return model
    
    def separate_vocals(self, audio_path, method='vocal'):
        """ë³´ì»¬ ë¶„ë¦¬ ìˆ˜í–‰"""
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            y, sr = librosa.load(audio_path, sr=self.sample_rate, mono=False)
            
            if y.ndim == 1:
                # ëª¨ë…¸ë¥¼ ìŠ¤í…Œë ˆì˜¤ë¡œ ë³€í™˜
                y = np.stack([y, y])
            
            # STFT ë³€í™˜
            stft_left = librosa.stft(y[0], n_fft=self.n_fft, hop_length=self.hop_length)
            stft_right = librosa.stft(y[1], n_fft=self.n_fft, hop_length=self.hop_length)
            
            # ë³µì†Œìˆ˜ë¥¼ ì‹¤ìˆ˜ë¶€, í—ˆìˆ˜ë¶€ë¡œ ë¶„ë¦¬
            magnitude_left = np.abs(stft_left)
            magnitude_right = np.abs(stft_right)
            phase_left = np.angle(stft_left)
            phase_right = np.angle(stft_right)
            
            # ìŠ¤í…Œë ˆì˜¤ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìƒì„±
            stereo_spec = np.stack([magnitude_left, magnitude_right], axis=-1)
            stereo_spec = np.expand_dims(stereo_spec.T, axis=0)  # (1, time, freq, 2)
            
            # AI ëª¨ë¸ ì ìš©
            if method == 'vocal' and self.vocal_model and self.is_loaded:
                mask = self.vocal_model.predict(stereo_spec, verbose=0)
            elif method == 'chorus' and self.chorus_model and self.is_loaded:
                mask = self.chorus_model.predict(stereo_spec, verbose=0)
            else:
                # í´ë°±: ì „í†µì ì¸ ë°©ë²•
                mask = self._traditional_separation(stereo_spec, method)
            
            # ë§ˆìŠ¤í¬ ì ìš©
            mask = mask[0].T  # (freq, time)
            mask = np.squeeze(mask)
            
            # ì•…ê¸° ë§ˆìŠ¤í¬ (ë³´ì»¬ ì œê±°ìš©)
            instrumental_mask = 1.0 - mask
            
            # ì¢Œìš° ì±„ë„ì— ë§ˆìŠ¤í¬ ì ìš©
            filtered_left = magnitude_left * instrumental_mask
            filtered_right = magnitude_right * instrumental_mask
            
            # ë³µì†Œìˆ˜ ì¬êµ¬ì„±
            stft_filtered_left = filtered_left * np.exp(1j * phase_left)
            stft_filtered_right = filtered_right * np.exp(1j * phase_right)
            
            # ISTFT ë³€í™˜
            y_left = librosa.istft(stft_filtered_left, hop_length=self.hop_length)
            y_right = librosa.istft(stft_filtered_right, hop_length=self.hop_length)
            
            # ìŠ¤í…Œë ˆì˜¤ ê²°í•©
            result = np.stack([y_left, y_right])
            
            # í›„ì²˜ë¦¬
            result = self._post_process(result, method)
            
            return result, self.sample_rate
            
        except Exception as e:
            print(f"ë¶„ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def _traditional_separation(self, stereo_spec, method):
        """ì „í†µì ì¸ ë¶„ë¦¬ ë°©ë²• (AI ëª¨ë¸ ì‹¤íŒ¨ì‹œ í´ë°±)"""
        # ì¤‘ì•™ ì±„ë„ ì¶”ì¶œ ê¸°ë°˜ ë§ˆìŠ¤í¬ ìƒì„±
        left_mag = stereo_spec[0, :, :, 0]
        right_mag = stereo_spec[0, :, :, 1]
        
        # ì¢Œìš° ì±„ë„ ì°¨ì´ë¥¼ ì´ìš©í•œ ë³´ì»¬ ê°ì§€
        center_channel = np.abs(left_mag - right_mag)
        side_channel = (left_mag + right_mag) / 2
        
        # ë™ì  ì„ê³„ê°’ ê³„ì‚°
        threshold = np.percentile(center_channel, 70)
        vocal_mask = (center_channel > threshold).astype(np.float32)
        
        if method == 'chorus':
            # ì½”ëŸ¬ìŠ¤ ëª¨ë“œì—ì„œëŠ” ë” ê°•í•œ ë§ˆìŠ¤í¬
            vocal_mask = np.where(vocal_mask > 0.3, 0.9, vocal_mask)
            vocal_mask = np.where(side_channel > np.percentile(side_channel, 60), 
                                vocal_mask * 1.2, vocal_mask)
        
        # ìŠ¤ë¬´ë”© ì ìš©
        from scipy import ndimage
        vocal_mask = ndimage.gaussian_filter(vocal_mask, sigma=1.0)
        
        return vocal_mask[np.newaxis, :, :, np.newaxis]
    
    def _post_process(self, audio, method):
        """ì˜¤ë””ì˜¤ í›„ì²˜ë¦¬"""
        # ì •ê·œí™”
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val * 0.95
        
        # ë…¸ì´ì¦ˆ ê²Œì´íŠ¸
        threshold = 0.001
        audio = np.where(np.abs(audio) < threshold, 0, audio)
        
        # ì†Œí”„íŠ¸ í´ë¦¬í•‘
        audio = np.tanh(audio * 0.9) * 0.9
        
        if method == 'chorus':
            # ì½”ëŸ¬ìŠ¤ ëª¨ë“œì—ì„œ ì¶”ê°€ì ì¸ ê³ ì£¼íŒŒ í•„í„°ë§
            from scipy.signal import butter, filtfilt
            nyquist = self.sample_rate / 2
            high_cutoff = 8000 / nyquist
            b, a = butter(4, high_cutoff, btype='low')
            
            for i in range(audio.shape[0]):
                audio[i] = filtfilt(b, a, audio[i])
        
        return audio

# ê¸€ë¡œë²Œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
vocal_separator = VocalSeparationModel()

@app.route('/', methods=['GET'])
def home():
    """í™ˆí˜ì´ì§€"""
    return '''
    <html>
    <head>
        <title>AI ë³´ì»¬ ë¦¬ë¬´ë²„ API</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; background: #f5f5f5; }
            .container { max-width: 800px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
            h1 { color: #333; text-align: center; }
            .status { background: #e8f5e8; padding: 15px; border-radius: 5px; margin: 20px 0; }
            .endpoint { background: #f8f9fa; padding: 15px; border-left: 4px solid #007bff; margin: 10px 0; }
            code { background: #f1f1f1; padding: 2px 5px; border-radius: 3px; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸµ AI ë³´ì»¬ ë¦¬ë¬´ë²„ API</h1>
            <div class="status">
                <strong>âœ… ì„œë²„ ìƒíƒœ:</strong> ì •ìƒ ë™ì‘<br>
                <strong>ğŸ¤– AI ëª¨ë¸:</strong> ''' + ('ë¡œë“œë¨' if vocal_separator.is_loaded else 'ë¡œë”© ì¤‘') + '''
            </div>
            
            <h2>ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸</h2>
            <div class="endpoint">
                <strong>GET /api/health</strong><br>
                ì„œë²„ ìƒíƒœ í™•ì¸
            </div>
            <div class="endpoint">
                <strong>POST /api/separate</strong><br>
                ìŒì„± ë¶„ë¦¬ ì²˜ë¦¬<br>
                Parameters: audio (file), method (vocal|chorus)
            </div>
            <div class="endpoint">
                <strong>GET /api/model-status</strong><br>
                AI ëª¨ë¸ ìƒíƒœ í™•ì¸
            </div>
            
            <p style="text-align: center; margin-top: 30px;">
                <a href="/index.html" style="background: #007bff; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px;">
                    ì›¹ ì¸í„°í˜ì´ìŠ¤ë¡œ ì´ë™ â†’
                </a>
            </p>
        </div>
    </body>
    </html>
    '''

@app.route('/api/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return jsonify({
        'status': 'healthy',
        'models_loaded': vocal_separator.is_loaded
    })

@app.route('/api/separate', methods=['POST'])
def separate_audio():
    """ìŒì„± ë¶„ë¦¬ API"""
    try:
        if 'audio' not in request.files:
            return jsonify({'error': 'ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤'}), 400
        
        audio_file = request.files['audio']
        method = request.form.get('method', 'vocal')  # 'vocal' or 'chorus'
        
        if audio_file.filename == '':
            return jsonify({'error': 'íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}), 400
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_input:
            audio_file.save(temp_input.name)
            temp_input_path = temp_input.name
        
        try:
            # ìŒì„± ë¶„ë¦¬ ìˆ˜í–‰
            separated_audio, sr = vocal_separator.separate_vocals(temp_input_path, method)
            
            # ê²°ê³¼ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_output:
                sf.write(temp_output.name, separated_audio.T, sr)
                temp_output_path = temp_output.name
            
            # íŒŒì¼ ì „ì†¡
            return send_file(
                temp_output_path,
                as_attachment=True,
                download_name=f'{method}_removed.wav',
                mimetype='audio/wav'
            )
        
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.unlink(temp_input_path)
                if 'temp_output_path' in locals():
                    os.unlink(temp_output_path)
            except:
                pass
    
    except Exception as e:
        return jsonify({'error': f'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}'}), 500

@app.route('/api/model-status', methods=['GET'])
def model_status():
    """ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    return jsonify({
        'vocal_model_loaded': vocal_separator.vocal_model is not None,
        'chorus_model_loaded': vocal_separator.chorus_model is not None,
        'ready': vocal_separator.is_loaded
    })

if __name__ == '__main__':
    print("ğŸµ AI ë³´ì»¬ ë¦¬ë¬´ë²„ ë°±ì—”ë“œ ì‹œì‘")
    print("í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ëª…ë ¹:")
    print("pip install -r requirements.txt")
    print()
    
    # ëª¨ë¸ ë¡œë“œ
    vocal_separator.load_models()
    
    # ì„œë²„ ì‹œì‘
    print("ì„œë²„ ì‹œì‘ ì¤‘...")
    print("ì›¹ ì¸í„°í˜ì´ìŠ¤: http://localhost:5000")
    print("API ë¬¸ì„œ: http://localhost:5000")
    
    app.run(host='0.0.0.0', port=5000, debug=True)keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(merged)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.